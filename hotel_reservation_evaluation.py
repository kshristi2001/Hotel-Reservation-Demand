# -*- coding: utf-8 -*-
"""Hotel_Reservation_Demand.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17rhC3UvthmgIFH9OAl0aZsfpTaT9N6-4
"""

#importing the necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

#loading the dataset
df = pd.read_csv('hotel_bookings.csv')
df.head(10)

# Strips leading and trailing whitespace from all string values in the dataframe
df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)

# Remove the columns and rows with null values.
df.drop(['company','agent','reservation_status_date'],axis = 1, inplace = True)
df.dropna(inplace = True)

#Convert categorical variables into dummy variables using one-hot encoding, dropping the first category to avoid multicollinearity issues.
dummy_df = pd.get_dummies(df, columns=['hotel','arrival_date_month', 'meal', 'country', 'deposit_type',
                                       'market_segment', 'distribution_channel', 'reserved_room_type',
                                       'assigned_room_type', 'customer_type', 'reservation_status'],
                                       drop_first=True)

#printing the dummy dataframe
dummy_df.head(10)

dummy_df.shape

# Select columns with dtype 'object' (strings), to make sure that we dont have any object variables
string_columns = dummy_df.select_dtypes(include=['object']).columns
print(string_columns)

#importing the necessary libraries for modelling
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score

# Separate features (X) and target variable (y) from the DataFrame
X = dummy_df.drop(columns=['is_canceled'])
y = dummy_df['is_canceled']

#Split the dataset into training and testing sets with 80% of the data for training and 20% for testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize a Random Forest classifier with 100 trees and a random state of 42
rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the Random Forest classifier on the training data
rf_clf.fit(X_train, y_train)

# Make predictions on the testing data using the trained classifier
rf_predictions = rf_clf.predict(X_test)

# Initialize the XGBClassifier
xgb_clf = XGBClassifier(learning_rate=0.1, n_estimators=100, random_state=42)

# Train the XGBClassifier
xgb_clf.fit(X_train, y_train)

# Make predictions using the XGBClassifier
xgb_predictions = xgb_clf.predict(X_test)

# Combine predictions using a simple voting mechanism
ensemble_predictions = []
for rf_pred, xgb_pred in zip(rf_predictions, xgb_predictions):
    ensemble_pred = rf_pred if rf_pred == xgb_pred else (rf_pred + xgb_pred) // 2
    ensemble_predictions.append(ensemble_pred)

# Evaluate accuracy of ensemble predictions
ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)
print("Ensemble Accuracy:", ensemble_accuracy)

# Compute the confusion matrix for the ensemble predictions
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test,ensemble_predictions)
print(cm)

# Generate a classification report for the ensemble predictions
from sklearn.metrics import classification_report

report = classification_report(y_test, ensemble_predictions)
print(report)

